{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c7fb39",
   "metadata": {},
   "source": [
    "# MultiFMClient\n",
    "\n",
    "MultiFMClient provides a uniform interface for interacting with LLMs from various providers. It adapts the official python libraries from providers such as Mistral, OpenAI, Meta, Anthropic, etc. to conform to the OpenAI chat completion interface.\n",
    "\n",
    "Below are some examples of how to use MultiFMClient to interact with different LLMs."
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-04T15:30:02.064319Z",
     "start_time": "2024-07-04T15:30:02.051986Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append('../aimodels')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4de3a24f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T15:31:12.914321Z",
     "start_time": "2024-07-04T15:31:12.796445Z"
    }
   },
   "source": [
    "from aimodels.client import MultiFMClient\n",
    "\n",
    "client = MultiFMClient()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Respond in Pirate English.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke\"},\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "adebd2f0b578a909",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T15:31:25.060689Z",
     "start_time": "2024-07-04T15:31:16.131205Z"
    }
   },
   "source": [
    "anthropic_claude_3_opus = \"anthropic:claude-3-opus-20240229\"\n",
    "\n",
    "response = client.chat.completions.create(model=anthropic_claude_3_opus, messages=messages)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrr, me bucko, 'ere be a jolly jest fer ye!\n",
      "\n",
      "What did th' pirate say on 'is 80th birthday? \"Aye matey!\"\n",
      "\n",
      "Ye see, it be a play on words, as \"Aye matey\" sounds like \"I'm eighty\". Har har har! 'Tis a clever bit o' pirate humor, if I do say so meself. Now, 'ow about ye fetch me a mug o' grog while I spin ye another yarn?\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6819ac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, here be a joke fer ye!\n",
      "\n",
      "Why did the pirate take a parrot on his ship?\n",
      "\n",
      "Because it were a hootin' good bird to have around, savvy? Aye, and it kept 'im company while he were swabbin' the decks! Arrrgh, I hope that made ye laugh, matey!\n"
     ]
    }
   ],
   "source": [
    "ollama_llama3 = \"ollama:llama3\"\n",
    "\n",
    "response = client.chat.completions.create(model=ollama_llama3, messages=messages, temperature=0.75)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T15:31:39.472675Z",
     "start_time": "2024-07-04T15:31:38.283368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mistral_7b = \"mistral:open-mistral-7b\"\n",
    "\n",
    "response = client.chat.completions.create(model=mistral_7b, messages=messages, temperature=0.2)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ],
   "id": "4a94961b2bddedbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arr matey, I've got a jest fer ye, if ye be ready for a laugh! Why did the pirate bring a clock to the island? Because he wanted to catch the time! Aye, that be a good one, I be thinkin'. Arrr!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "611210a4dc92845f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
